{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day_2_Live_Demo_1_Basic_Data_Cleaning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Basic Data Cleaning**"],"metadata":{"id":"sFcYiVhbweM9"}},{"cell_type":"markdown","source":["In this tutorial, you will learn:\n","\n","* How to identify and remove column variables that only have a single value.\n","* How to identify and consider column variables with very few unique values.\n","* How to identify and remove rows that contain duplicate observations.\n","\n","Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."],"metadata":{"id":"T-xjYFrCQhmD"}},{"cell_type":"markdown","source":["#Messy Dataset\n","\n","\n","Breast cancer dataset classifies breast cancer\n","patient as either a recurrence or no recurrence of cancer. \n","\n","```\n","Number of Instances: 289\n","Number of Attributes: 9 + the class attribute\n","Attribute Information:\n","   1. Class: no-recurrence-events, recurrence-events\n","   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n","   3. menopause: lt40, ge40, premeno.\n","   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n","   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n","   6. node-caps: yes, no.\n","   7. deg-malig: 1, 2, 3.\n","   8. breast: left, right.\n","   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n","  10. irradiat:\tyes, no.\n","Missing Attribute Values: (denoted by \"?\")\n","   Attribute #:  Number of instances with missing values:\n","   6.             8\n","   9.             1.\n","Class Distribution:\n","    1. no-recurrence-events: 201 instances\n","    2. recurrence-events: 85 instances \n","```\n","You can learn more about the dataset here:\n","* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n","* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n","\n","\n","The messy dataset was modified from Breast Cancer Dataset.\n"],"metadata":{"id":"giZ-tn6J7ktc"}},{"cell_type":"markdown","source":["###Download messy data file"],"metadata":{"id":"ibIiSW0g-r4n"}},{"cell_type":"code","source":["!wget \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\" -O messy_data.csv\n","!head messy_data.csv"],"metadata":{"id":"4MvsLbWB-hSu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550186898,"user_tz":240,"elapsed":755,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"05b94bdb-1257-4d0d-80f8-9a627294b496"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-06 21:16:26--  https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 25496 (25K) [text/plain]\n","Saving to: ‘messy_data.csv’\n","\n","messy_data.csv      100%[===================>]  24.90K  --.-KB/s    in 0.002s  \n","\n","2022-06-06 21:16:26 (9.80 MB/s) - ‘messy_data.csv’ saved [25496/25496]\n","\n","'40-49','premeno','15-19','0-2','yes',4','3','right','left_up','no','recurrence-events'\n","'50-59','ge40','15-19','0-2','no',4','1','right','central','no','no-recurrence-events'\n","'50-59','ge40','35-39','0-2','no',4','2','left','left_low','no','recurrence-events'\n","'40-49','premeno','35-39','0-2','yes',4','3','right','left_low','yes','no-recurrence-events'\n","'40-49','premeno','30-34','3-5','yes',4','2','left','right_up','no','recurrence-events'\n","'50-59','premeno','25-29','3-5','no',4','2','right','left_up','yes','no-recurrence-events'\n","'50-59','ge40','40-44','0-2','no',4','3','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','10-14','0-2','no',4','2','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','0-4','0-2','no',4','2','right','right_low','no','no-recurrence-events'\n","'40-49','ge40','40-44','15-17','yes',4','2','right','left_up','yes','no-recurrence-events'\n"]}]},{"cell_type":"markdown","source":["#Identify Columns That Contain a Single Value\n"],"metadata":{"id":"sRe1fc_I-MQw"}},{"cell_type":"code","source":["# summarize the number of unique values for each column using pandas\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column using nunique()\n","print(\"Shape of messy data: \", df.shape)\n","print(\"Column\\t#Unique values \")\n","print(df.nunique())"],"metadata":{"id":"Fz3fYIjp_ea4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550216146,"user_tz":240,"elapsed":722,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"746d2b5f-9bfc-4301-f480-786c27b7e521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of messy data:  (289, 11)\n","Column\t#Unique values \n","0      6\n","1      3\n","2     11\n","3      7\n","4      2\n","5      1\n","6      3\n","7      2\n","8      5\n","9      2\n","10     2\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["We can see that column index 5 only has a single value and should be removed."],"metadata":{"id":"OpjQlWtH_qcK"}},{"cell_type":"markdown","source":["#Delete columns that contain a single value"],"metadata":{"id":"1bPtTttf_vga"}},{"cell_type":"code","source":["# delete columns with a single unique value\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if v == 1]\n","print(to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"738Abx8f_0-I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550250053,"user_tz":240,"elapsed":271,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"eac853aa-b3e3-4c44-a29f-30136deff10d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","[5]\n","(289, 10)\n"]}]},{"cell_type":"markdown","source":["#Identify columns that have very few values"],"metadata":{"id":"i5bW7pFsAI3N"}},{"cell_type":"code","source":["from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column\n","print(\"Column, Count, <1%\")\n","for i, v in enumerate(df.nunique()):\n","  # Percent of number of unique values across rows\n","  percentage = float(v) / df.shape[0] * 100\n","  if percentage < 1:\n","    print('%d, %d, %.1f%%' % (i, v, percentage))"],"metadata":{"id":"qW3BwXIOFbfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550323133,"user_tz":240,"elapsed":262,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"eb91a6d7-c77b-4a29-8200-a74a6a219ceb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Column, Count, <1%\n","4, 2, 0.7%\n","5, 1, 0.3%\n","7, 2, 0.7%\n","9, 2, 0.7%\n","10, 2, 0.7%\n"]}]},{"cell_type":"markdown","source":["#Drop columns with unique values less than 1 percent of rows"],"metadata":{"id":"vnjw39eIF2kD"}},{"cell_type":"code","source":["# delete columns where number of unique values is less than 1% of the rows\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0] * 100) < 1]\n","print(\"Columns to delete: \", to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"KiPY_mBqGGeQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550542844,"user_tz":240,"elapsed":282,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"9e7e4fd6-aac5-425d-c9d7-477609f817ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","Columns to delete:  [4, 5, 7, 9, 10]\n","(289, 6)\n"]}]},{"cell_type":"markdown","source":["#Identify rows that contain duplicate data"],"metadata":{"id":"fOPZfL-JPe7U"}},{"cell_type":"code","source":["# locate rows of duplicate data\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# calculate duplicates\n","dups = df.duplicated()\n","# report if there are any duplicates\n","print(\"Any duplicates? \", dups.any())\n","# list all duplicate rows\n","print(\"Duplicated rows:\")\n","print(df[dups])"],"metadata":{"id":"WFrSFoZRPnpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550562164,"user_tz":240,"elapsed":378,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"76c6c6ed-b97c-468e-f545-6a3fddb58d3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Any duplicates?  True\n","Duplicated rows:\n","          0          1        2      3      4   5    6        7           8   \\\n","17   '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","27   '40-49'  'premeno'  '10-14'  '0-2'   'no'  4'  '1'  'right'   'left_up'   \n","44   '30-39'  'premeno'  '15-19'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","65   '50-59'     'ge40'  '15-19'  '0-2'   'no'  4'  '1'  'right'   'central'   \n","117  '60-69'     'ge40'  '10-14'  '0-2'   'no'  4'  '1'   'left'   'left_up'   \n","178  '40-49'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'  'right'  'left_low'   \n","190  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'right_up'   \n","214  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","217  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","221  '50-59'     'ge40'  '20-24'  '0-2'   'no'  4'  '3'   'left'   'left_up'   \n","237  '30-39'     'lt40'  '15-19'  '0-2'   'no'  4'  '3'  'right'   'left_up'   \n","240  '50-59'     'ge40'  '40-44'  '6-8'  'yes'  4'  '3'   'left'  'left_low'   \n","246  '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","268  '30-39'  'premeno'  '35-39'  '0-2'   'no'  4'  '3'   'left'  'left_low'   \n","270  '60-69'     'ge40'  '20-24'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","287  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'  'right_up'   \n","288  '50-59'     'ge40'  '40-44'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","\n","        9                       10  \n","17    'no'  'no-recurrence-events'  \n","27    'no'  'no-recurrence-events'  \n","44    'no'  'no-recurrence-events'  \n","65    'no'  'no-recurrence-events'  \n","117   'no'  'no-recurrence-events'  \n","178   'no'     'recurrence-events'  \n","190   'no'     'recurrence-events'  \n","214   'no'  'no-recurrence-events'  \n","217   'no'  'no-recurrence-events'  \n","221   'no'  'no-recurrence-events'  \n","237   'no'  'no-recurrence-events'  \n","240  'yes'     'recurrence-events'  \n","246   'no'  'no-recurrence-events'  \n","268   'no'     'recurrence-events'  \n","270   'no'  'no-recurrence-events'  \n","287   'no'  'no-recurrence-events'  \n","288   'no'  'no-recurrence-events'  \n"]}]},{"cell_type":"markdown","source":["#Delete rows that contain duplicate data"],"metadata":{"id":"noMvFc6AP-53"}},{"cell_type":"code","source":["# delete rows of duplicate data from the dataset\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# delete duplicate rows\n","df.drop_duplicates(inplace=True)\n","print(df.shape)"],"metadata":{"id":"G4VBQ_dIQEes","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654550585500,"user_tz":240,"elapsed":299,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"25b32672-d8b8-4a01-c7e5-00d1ff274053"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","(272, 11)\n"]}]}]}